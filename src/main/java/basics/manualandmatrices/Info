Defect Slippage
Definition:
Defect slippage occurs when bugs escape QA testing and are found after release (UAT, Production).

Example:
QA tested login feature but missed a security bug ‚Üí customer finds it after release.

‚úÖ Defect Leakage
Definition:
Defect leakage happens when bugs escape from one testing phase to the next testing phase.
‚û°Ô∏è Slippage = escaped to PRODUCTION
‚û°Ô∏è Leakage = escaped to NEXT PHASE (e.g., Unit ‚Üí Integration, QA ‚Üí UAT)

Example:
Bug missed in System Testing, found in UAT = defect leakage.
‚≠ê Difference (Simple)
Term	Escaped To
Defect Leakage	Escapes to next test phase
Defect Slippage	Escapes to production


‚úÖ Manual Testing Types
1Ô∏è‚É£ Black Box Testing

Tester doesn‚Äôt know internal code.
Examples: Functional testing, regression testing, system testing.

2Ô∏è‚É£ White Box Testing
Tester knows internal logic, paths.

3Ô∏è‚É£ Grey Box Testing
Partial internal knowledge.


Common Manual Functional Testing Types:
Type	Purpose
Smoke Testing	Quick check: ‚ÄúIs build stable?‚Äù
Sanity Testing	After fixes: ‚ÄúDoes it work now?‚Äù
Regression Testing	Ensure old functionality still works
Re-testing	Verify bug fixes
Functional Testing	Test features as per requirements
Integration Testing	Test interactions between modules
System Testing	Complete system test
UAT Testing	User/business testing
Ad-hoc Testing	Informal, no documentation
üü¶ Boundary Value Analysis (BVA)
Definition:

Test boundaries of input ranges because bugs often occur at edges.

Example:

Age must be 18‚Äì60
Test: 17, 18, 60, 61

üü¶ Equivalence Partitioning (EP)
Definition:
Divide input into valid and invalid partitions; test one input from each.

Example:
Age 18‚Äì60
Valid: 18‚Äì60 ‚Üí test 30
Invalid partition 1: <18 ‚Üí test 10
Invalid partition 2: >60 ‚Üí test 70

üü¶ Experience-Based Testing
Definition:
Testing based on tester‚Äôs experience, intuition, domain knowledge.

Examples:
Guessing where bugs usually occur
Prior failed modules

Risk areas

üü¶ Ad-hoc Testing
Definition:

Informal, unstructured testing without test cases.
Used to quickly explore and find defects.

Example: Randomly trying combinations, invalid inputs, quick checks.

‚≠ê Root Cause Analysis (RCA) Techniques

RCA helps identify why the defect occurred.

Common RCA methods:
1Ô∏è‚É£ 5 Whys

Ask ‚ÄúWhy?‚Äù repeatedly until you find the root cause.

2Ô∏è‚É£ Fishbone Diagram (Ishikawa)

Categorizes possible causes:
People
Process
Tools
Environment
Requirements

3Ô∏è‚É£ Pareto Analysis (80/20 Rule)
80% issues come from 20% causes.

4Ô∏è‚É£ Failure Mode & Effects Analysis (FMEA)
Analyze failure types ‚Üí severity ‚Üí detectability.

‚≠ê Definition of Done (DoD)
Definition:
A checklist of criteria that must be met before a task/user story is considered complete.

Example DoD:
Code completed
Unit tested
Functionally tested
Regression tested
Documentation updated
Accepted by PO

DoD ensures consistent quality across sprints.

‚≠ê Shift Left Testing
Definition:

Testing earlier in the SDLC to catch defects early.
Why?
Reduced cost
Faster delivery
Higher quality
Examples:

Testers involved in requirement discussions
Early test case creation
Early automation
API contract testing before UI is ready

üìå Summary Sheet (quick memory)
Concept	Meaning
Defect Slippage	Bug found in production
Defect Leakage	Bug found in next test phase
BVA	Test boundaries
EP	Divide input into partitions
Ad-hoc	Informal testing
Experience-based	Intuition-based testing
RCA	Finding root cause of defect
Definition of Done	Checklist for completion
Shift Left	Test earlier to reduce defects






NEXT TOPICS:
SDLC Models
STLC Phases
Agile & Scrum Concepts
Test Case Design Techniques
Severity vs Priority
Bug/Defect Life Cycle

Let‚Äôs go step by step.

‚úÖ 1. SDLC Models (Software Development Life Cycle)
‚≠ê 1. Waterfall Model

Linear, sequential
Each phase completed before next
Requirements need to be stable

‚û°Ô∏è Good: Simple, structured
‚û°Ô∏è Bad: No flexibility, late testing

‚≠ê 2. V-Model (Validation & Verification)

Testing activities happen in parallel to development phases

Test planning starts early

‚û°Ô∏è Good: Test early
‚û°Ô∏è Bad: Still rigid like waterfall

‚≠ê 3. Iterative / Incremental Model

Build software in pieces (increments)

Each build adds new functionality

‚û°Ô∏è Good: Early delivery
‚û°Ô∏è Bad: Requires good planning

‚≠ê 4. Agile Model (Most used today)
Iterative + incremental
Work in small cycles (sprints)
Continuous feedback & improvement

‚û°Ô∏è Good: Flexible, customer involvement
‚û°Ô∏è Bad: Needs strong communication

‚úÖ 2. STLC (Software Testing Life Cycle)
STLC Phases:

Requirement Analysis
Understand requirements
Identify testable items
Identify test types
Test Planning

Estimate effort

Resource planning

Risk analysis

Strategy preparation

Test Case Design
Create test cases
Prepare test data
Environment Setup
Get hardware/software ready

Deploy build

Test Execution

Run test cases

Log defects

Defect Reporting

Raise bugs with details

Test Cycle Closure

Test summary report

Metrics

Lessons learned

‚úÖ 3. Agile & Scrum Concepts
‚≠ê Agile Principles

Early & continuous delivery
Welcome changes
Frequent delivery
Communication
Working software > documentation

‚≠ê Scrum Terminology
Term	Meaning
Product Owner (PO)	Defines requirements, sets priority
Scrum Master	Removes blockers, facilitator
Development Team	Dev + QA team
Sprint	1‚Äì4 week time-box
Sprint Planning	Decide sprint work
Daily Scrum	15-min stand-up meeting
Sprint Review	Show completed work
Sprint Retrospective	Improve process
‚≠ê Scrum Artifacts
Artifact	Meaning
Product Backlog	List of all requirements
Sprint Backlog	Tasks for the sprint
Increment	Completed + tested product
‚úÖ 4. Test Case Design Techniques

You already saw BVA, EP, Adhoc, Experience-based.
Here are others:

‚≠ê 1. Decision Table Testing

Used when there are multiple conditions and multiple outcomes.

Example: Login with combinations of username/password valid/invalid.

‚≠ê 2. State Transition Testing

Used when output depends on previous state.

Example:

ATM: card inserted ‚Üí PIN ‚Üí withdraw

User states: Active ‚Üí Locked ‚Üí Disabled

‚≠ê 3. Use Case Testing

User-flow based testing.

‚≠ê 4. Error Guessing

Based on experience, guess where bugs may occur.

‚úÖ 5. Severity vs Priority
Term	Meaning	Decided By
Severity	Impact of defect on system	Tester
Priority	Order in which defect must be fixed	Product Owner / Manager
‚≠ê Examples

App crashes ‚Üí High severity

Typo on homepage ‚Üí Low severity but may be High priority

‚úÖ 6. Bug/Defect Life Cycle
Typical defect states:

New

Assigned

Open

Fixed

Retest

Verified

Closed

If not fixed:

Rejected

Duplicate

Deferred

Not a bug

üìå Quick Summary Sheet
‚≠ê SDLC

Waterfall, V-model, Iterative, Agile

‚≠ê STLC

Requirement ‚Üí Planning ‚Üí Design ‚Üí Setup ‚Üí Execution ‚Üí Closure

‚≠ê Agile/Scrum

Sprint, PO, SM, Review, Retro, Backlogs

‚≠ê Test Techniques

BVA, EP, Decision table, State transition, Use-case, Adhoc, Experience-based

‚≠ê Severity vs Priority

Impact vs urgency

‚≠ê Bug Life Cycle

New ‚Üí Assigned ‚Üí Open ‚Üí Fixed ‚Üí Retest ‚Üí Verified ‚Üí Closed



NEXT TOPICS:

Testing Metrics

Performance Testing Basics

Security Testing Basics

Test Scenario vs Test Case vs Test Condition

Types of Test Environments

Top Manual Testing Interview Questions (with answers)

Let‚Äôs begin.

‚úÖ 1. Testing Metrics (Very Important!)

Metrics help measure testing progress, quality, and effectiveness.

‚≠ê 1. Test Coverage

How much of the application is tested.

Formula:

Test Coverage = (Number of requirements tested / Total requirements) √ó 100

‚≠ê 2. Defect Density

Number of defects per size unit (module size, LOC, function points).

Formula:

Defect Density = Defects / Size

‚≠ê 3. Defect Removal Efficiency (DRE)

Measures how many defects were caught before release.

DRE = (Defects found during testing / Total defects) √ó 100

‚≠ê 4. Test Execution Metrics

Passed test cases

Failed test cases

Blocked test cases

Not executed test cases

‚≠ê 5. Defect Leakage %

Defects escaped to UAT or Production.

Leakage % = (Defects leaked / Total defects) √ó 100

‚úÖ 2. Performance Testing Basics
‚≠ê Types of performance testing:
Type	Purpose
Load Testing	Check system behavior under expected load
Stress Testing	Push system beyond limits
Spike Testing	Sudden large increase in load
Endurance / Soak Testing	Test long-duration stability
Scalability Testing	Increase users gradually
‚≠ê Key performance metrics:

Response time

Throughput

Latency

Error rate

CPU/memory usage

‚úÖ 3. Security Testing Basics
‚≠ê Common types:

Authentication testing

login, password rules, MFA

Authorization testing

users should not access others‚Äô data

Input validation testing

SQL injection

XSS

Session management

timeout

cookie testing

Data encryption testing

https, TLS

Broken access control

manually changing URLs or IDs

‚úÖ 4. Test Scenario vs Test Case vs Test Condition
‚≠ê Test Scenario

High-level end-user activity.

Example:

‚ÄúVerify user can log into the application.‚Äù

‚≠ê Test Case

Detailed steps + expected results.

Example:

Enter username

Enter password

Click login

Expected: dashboard opens

‚≠ê Test Condition

Requirement or rule to be tested.

Example:

User must enter valid credentials

Password must be encrypted

‚úÖ 5. Types of Test Environments
‚≠ê 1. DEV Environment

Developers test their code

Unstable, frequently changing

‚≠ê 2. QA / TEST Environment

QA executes test cases

Stable build provided for testing

‚≠ê 3. UAT Environment

Client tests application

Real-world business validation

‚≠ê 4. Pre-Prod Environment

Final testing before deployment

Mirrors production setup

‚≠ê 5. Production Environment

Live system used by end users

‚úÖ 6. Top Manual Testing Interview Questions (with answers)
‚≠ê 1. What is the difference between verification and validation?

Verification: Are we building the product right?

Validation: Are we building the right product?

‚≠ê 2. What is the difference between functional and non-functional testing?

Functional: tests features, behavior

Non-functional: performance, security, usability

‚≠ê 3. What is sanity testing?

Sanity = Quick testing of a specific module after bug fixes.

‚≠ê 4. What is regression testing?

Testing to ensure new changes didn't break existing features.

‚≠ê 5. What are entry and exit criteria?

Entry criteria: what must be ready before testing starts

Exit criteria: what must be done before testing ends

‚≠ê 6. What would you do if the developer rejects your bug?

Provide logs, screenshots, steps

Reproduce with developer

Compare with requirement

Escalate if needed

‚≠ê 7. What are high severity & low priority defects?

App crash on a rare page = high severity, low priority.

üéØ Summary Sheet

Test metrics measure effectiveness

Performance testing checks speed & stability

Security testing checks risks & vulnerabilities

Test scenario = high level

Test case = detailed

Test environment types: DEV ‚Üí QA ‚Üí UAT ‚Üí PROD

Common interview questions covered